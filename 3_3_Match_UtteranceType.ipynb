{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtq6YyiuCrohYPuXuYd9Gl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Janina712/RhythmMetrics_Duration/blob/main/3_3_Match_UtteranceType.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **0. Imports & Set-Up**"
      ],
      "metadata": {
        "id": "3oUgMDL2WJ1N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vg6yXbtVz2t"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import random as random\n",
        "import os\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "7F_KdH5fV40n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/ATAS_Plus/Duration_Metrics/2.BreathGroups_Assigned/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUUziL27V4vp",
        "outputId": "0326fc9d-7ea5-4f2d-ee29-da11b124b62f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ATAS_Plus/Duration_Metrics/2.BreathGroups_Assigned\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reading = pd.read_excel(\"reading_TextGrid_comb_BG_loop.xlsx\")\n",
        "frog = pd.read_excel(\"frog_TextGrid_comb_BG_loop.xlsx\")"
      ],
      "metadata": {
        "id": "o_j--tqyV4sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IDs by condition\n",
        "IDs_reading = ['24fa']\n",
        "IDs_frog = ['24fa']"
      ],
      "metadata": {
        "id": "71pXJPcZV4p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IDs by group\n",
        "IDs_pws = ['24fa']\n",
        "control_IDs = []\n",
        "\n",
        "# combine both groups\n",
        "IDs_col = pd.DataFrame(columns =['ID'])\n",
        "IDs_col['ID'] =['24fa']"
      ],
      "metadata": {
        "id": "1Xtew84ZV4jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Define Functions**"
      ],
      "metadata": {
        "id": "zpfM9i5WWRGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1 Assign Participant Groups**"
      ],
      "metadata": {
        "id": "Onbw-CdKWdKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_group(df):\n",
        "  group = pd.DataFrame(index = range(len(df)),columns=[\"Group\"])\n",
        "\n",
        "  for i in range(0,len(df)):\n",
        "    if \"_\" in df[\"ID\"][i]:\n",
        "      group[\"Group\"][i] = \"Control\"\n",
        "    else:\n",
        "      group[\"Group\"][i] = \"PWS\"\n",
        "\n",
        "  df_out = pd.concat([ group, df], axis=1)\n",
        "  df_out = df_out[df_out.Type != \"silence\"]\n",
        "  df_out.index = range(len(df_out.index))\n",
        "  df_out.drop(['Unnamed: 0'], axis=1 , inplace = True)\n",
        "  return(df_out)"
      ],
      "metadata": {
        "id": "8Vf88GIRWUl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Count Vowels**"
      ],
      "metadata": {
        "id": "3cuJpjEuWlUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_vowels(df, condition):\n",
        "  if 'Syllables' in df.columns:\n",
        "    df = df.drop(columns=['Syllables'])\n",
        "  df_vowels = df[df[\"Type\"]  == \"vowel\"]\n",
        "  df_vowels.index = range(len(df_vowels.index))\n",
        "\n",
        "  # reading or interview condition\n",
        "  if condition == \"frog\":\n",
        "    IDs_here = IDs_frog\n",
        "  else:\n",
        "    IDs_here = IDs_reading\n",
        "\n",
        "  syll_col = pd.DataFrame()  ## initialize group-level dataframe\n",
        "  for ID in IDs_here: ## loop over participnts\n",
        "    syll_current_ID = pd.DataFrame()   ## initialize participant-level dataframe\n",
        "    subset_sounds = df[df[\"ID\"] == ID]  # get subset of sound dataframe that corresponds to current participant\n",
        "    subset_sounds.index = range(len(subset_sounds.index)) # reset index\n",
        "    subset_vowels = subset_sounds[subset_sounds[\"Type\"] == \"vowel\"]  # get subset of sound dataframe that corresponds to current participant\n",
        "    subset_vowels.index = range(len(subset_vowels.index)) # reset index\n",
        "    syll = subset_vowels[\"Breath.Group\"].value_counts().sort_index() # count how often a certain Breath group occurs for this participant\n",
        "    syll.index = range(len(syll.index)) # reset index\n",
        "    for a in range (0,len(syll)): # go through all breath groups that this participant produced\n",
        "      syll_current_BG = pd.DataFrame()  ## initialize BG-level dataframe\n",
        "      syll_current_BG = pd.DataFrame(np.repeat(syll.iloc[a], syll.iloc[a], axis=0)) #replicate the sum sum times\n",
        "      syll_current_ID = syll_current_ID.append([syll_current_BG], ignore_index = True) # add BG-level dataframe to participant-level dataframe\n",
        "    syll_col = syll_col.append([syll_current_ID], ignore_index = True) # add participant-level dataframe to group-level dataframe\n",
        "\n",
        "  df_vowels = pd.concat([df_vowels, syll_col], axis=1)\n",
        "  df_vowels.rename(columns = {'Syllables':'Unmached_Vowels'}, inplace = True)\n",
        "  df_vowels.rename(columns = {0:'Syllables'}, inplace = True) # rename new column\n",
        "  pre_df_vowel_avg = df_vowels.groupby(\"Group\").mean()    ########### average counting 13 13 times\n",
        "\n",
        "  return(df_vowels, pre_df_vowel_avg)"
      ],
      "metadata": {
        "id": "icEn7mMiWUj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.3 Count Consonants**"
      ],
      "metadata": {
        "id": "UcnBueCoWrN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_consonants(df, condition):\n",
        "  if 'Consonants' in df.columns:\n",
        "    df = df.drop(columns=['Consonants'])\n",
        "  df_consonants = df[df[\"Type\"]  == \"consonant\"]\n",
        "  df_consonants.index = range(len(df_consonants.index))\n",
        "\n",
        "  # reading or interview condition\n",
        "  if condition == \"frog\":\n",
        "    IDs_here = IDs_frog\n",
        "  else:\n",
        "    IDs_here = IDs_reading\n",
        "\n",
        "  con_col = pd.DataFrame()  ## initialize group-level dataframe\n",
        "  for ID in IDs_here: ## loop over participnts\n",
        "    con_current_ID = pd.DataFrame()   ## initialize participant-level dataframe\n",
        "    subset_sounds = df[df[\"ID\"] == ID]  # get subset of sound dataframe that corresponds to current participant\n",
        "    subset_sounds.index = range(len(subset_sounds.index)) # reset index\n",
        "    subset_cons = subset_sounds[subset_sounds[\"Type\"] == \"consonant\"]  # get subset of sound dataframe that corresponds to current participant\n",
        "    subset_cons.index = range(len(subset_cons.index)) # reset index\n",
        "    con = subset_cons[\"Breath.Group\"].value_counts().sort_index() # count how often a certain Breath group occurs for this participant\n",
        "    con.index = range(len(con.index)) # reset index\n",
        "    for a in range (0,len(con)): # go through all breath groups that this participant produced\n",
        "      con_current_BG = pd.DataFrame()  ## initialize BG-level dataframe\n",
        "      con_current_BG = pd.DataFrame(np.repeat(con.iloc[a], con.iloc[a], axis=0)) #replicate the sum sum times\n",
        "      con_current_ID = con_current_ID.append([con_current_BG], ignore_index = True) # add BG-level dataframe to participant-level dataframe\n",
        "    con_col = con_col.append([con_current_ID], ignore_index = True) # add participant-level dataframe to group-level dataframe\n",
        "\n",
        "  df_consonants = pd.concat([df_consonants, con_col], axis=1)\n",
        "  df_consonants.rename(columns = {'Consonants':'Unmatched_Cons'}, inplace = True)\n",
        "  df_consonants.rename(columns = {0:'Consonants'}, inplace = True) # rename new column\n",
        "  pre_df_consonant_avg = df_consonants.groupby(\"Group\").mean()    ########### average counting 13 13 times\n",
        "\n",
        "  return(df_consonants, pre_df_consonant_avg)"
      ],
      "metadata": {
        "id": "O9rp00kvWUiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.4 Average Vowel Count Per Participant**"
      ],
      "metadata": {
        "id": "32nm09RWXMaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def participant_vowel_avg(df, condition):\n",
        "  IDs_out = []\n",
        "  group_list = []\n",
        "  if len(df) == 0:\n",
        "    df_participant_vowel_avg = pd.DataFrame()\n",
        "    df_participant_vowel_avg[\"Group\"] = []\n",
        "    df_participant_vowel_avg[\"ID\"] = []\n",
        "    df_participant_vowel_avg[\"Syllables\"] = []\n",
        "\n",
        "  else:\n",
        "    if condition == \"frog\":\n",
        "      IDs_here = IDs_frog\n",
        "    else:\n",
        "      IDs_here = IDs_reading\n",
        "\n",
        "    IDs_here_pws = []\n",
        "    for ID in IDs_here:\n",
        "      if ID in IDs_pws:\n",
        "        IDs_here_pws.append(ID)\n",
        "\n",
        "    n = -1\n",
        "    avg_col = pd.DataFrame(columns=[\"Syllables\"])   # Syllables\n",
        "    for ID in IDs_here_pws: ## loop over participnts\n",
        "      n = n + 1\n",
        "      subset_BGs = df[df[\"ID\"] == ID]  # get subset of sound dataframe that corresponds to current participant\n",
        "      subset_BGs.index = range(len(subset_BGs.index)) # reset index\n",
        "      subset_fluent = df[df[\"FluencyStatus\"] == \"fluent\"]  # get subset of sound dataframe that corresponds to current participant\n",
        "      subset_fluent.index = range(len(subset_BGs.index)) # reset index\n",
        "      BG_avg = subset_fluent.groupby(\"Breath.Group\").mean()\n",
        "      subj_avg_fluent = BG_avg[\"Syllables\"].mean()\n",
        "      avg_col.loc[n] = subj_avg_fluent\n",
        "\n",
        "      subset_disfluent = df[df[\"FluencyStatus\"] == \"disfluent\"]  # get subset of sound dataframe that corresponds to current participant\n",
        "      subset_disfluent.index = range(len(subset_disfluent.index)) # reset index\n",
        "      if len(subset_disfluent) == 0:\n",
        "        avg_col.loc[n+1] = 'NAN'\n",
        "        n = n + 1\n",
        "      else:\n",
        "        BG_avg = subset_fluent.groupby(\"Breath.Group\").mean()\n",
        "        subj_avg_disfluent = BG_avg[\"Syllables\"].mean()\n",
        "        avg_col.loc[n+1] = subj_avg_disfluent\n",
        "        n = n + 1\n",
        "\n",
        "\n",
        "      group_col = pd.DataFrame(columns=['FluencyStatus'])\n",
        "      IDs_col_here = pd.DataFrame(columns=['ID'])\n",
        "      for i in range(0,len(IDs_here_pws)):\n",
        "        group_list.append(\"Fluent\")\n",
        "        group_list.append('Disfluent')\n",
        "\n",
        "      IDs_out.append(ID)\n",
        "      IDs_out.append(ID)\n",
        "\n",
        "    group_col[\"FluencyStatus\"] = group_list\n",
        "    IDs_col_here[\"ID\"] = IDs_out\n",
        "\n",
        "    df_participant_vowel_avg = pd.concat([group_col, IDs_col_here, avg_col], axis=1)\n",
        "\n",
        "  return(df_participant_vowel_avg)"
      ],
      "metadata": {
        "id": "nxB6BdDTWUcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.5 Average Consonant Count Per Participant**"
      ],
      "metadata": {
        "id": "fSHP-t70XHEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def participant_consonant_avg(df, condition):\n",
        "  IDs_out = []\n",
        "  group_list = []\n",
        "  if len(df) == 0:\n",
        "    df_participant_cons_avg = pd.DataFrame()\n",
        "    df_participant_cons_avg[\"Group\"] = []\n",
        "    df_participant_cons_avg[\"ID\"] = []\n",
        "    df_participant_cons_avg[\"Syllables\"] = []\n",
        "  else:\n",
        "    if condition == \"frog\":\n",
        "      IDs_here = IDs_frog\n",
        "    else:\n",
        "      IDs_here = IDs_reading\n",
        "\n",
        "    IDs_here_pws = []\n",
        "    for ID in IDs_here:\n",
        "      if ID in IDs_pws:\n",
        "        IDs_here_pws.append(ID)\n",
        "\n",
        "\n",
        "    n = -1\n",
        "    avg_col = pd.DataFrame(columns=[\"Consonants\"])   # Syllables\n",
        "    for ID in IDs_here_pws: ## loop over participnts\n",
        "      n = n + 1\n",
        "      subset_BGs = df[df[\"ID\"] == ID]  # get subset of sound dataframe that corresponds to current participant\n",
        "      subset_BGs.index = range(len(subset_BGs.index)) # reset index\n",
        "      subset_fluent = df[df[\"FluencyStatus\"] == \"fluent\"]  # get subset of sound dataframe that corresponds to current participant\n",
        "      subset_fluent.index = range(len(subset_BGs.index)) # reset index\n",
        "      BG_avg = subset_fluent.groupby(\"Breath.Group\").mean()\n",
        "      subj_avg_fluent = BG_avg[\"Consonants\"].mean()\n",
        "      avg_col.loc[n] = subj_avg_fluent\n",
        "\n",
        "      subset_disfluent = df[df[\"FluencyStatus\"] == \"disfluent\"]  # get subset of sound dataframe that corresponds to current participant\n",
        "      subset_disfluent.index = range(len(subset_disfluent.index)) # reset index\n",
        "      if len(subset_disfluent) == 0:\n",
        "        avg_col.loc[n+1] = 'NAN'\n",
        "        n = n + 1\n",
        "      else:\n",
        "        BG_avg = subset_fluent.groupby(\"Breath.Group\").mean()\n",
        "        subj_avg_disfluent = BG_avg[\"Consonants\"].mean()\n",
        "        avg_col.loc[n+1] = subj_avg_disfluent\n",
        "        n = n + 1\n",
        "\n",
        "      group_col = pd.DataFrame(columns=['FluencyStatus'])\n",
        "      IDs_col_here = pd.DataFrame(columns=['ID'])\n",
        "      for i in range(0,len(IDs_here_pws)):\n",
        "        group_list.append(\"Fluent\")\n",
        "        group_list.append('Disfluent')\n",
        "\n",
        "      IDs_out.append(ID)\n",
        "      IDs_out.append(ID)\n",
        "\n",
        "    group_col[\"FluencyStatus\"] = group_list\n",
        "    IDs_col_here[\"ID\"] = IDs_out\n",
        "\n",
        "    df_participant_cons_avg = pd.concat([group_col, IDs_col_here, avg_col], axis=1)\n",
        "\n",
        "  return(df_participant_cons_avg)"
      ],
      "metadata": {
        "id": "sXQqLMeNUnKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.6 Compare Vowel and Consonant Counts Across Utterance Types**"
      ],
      "metadata": {
        "id": "j858EJeuXaST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_groups(df_vowels_fluent, df_vowels_disfluent, df_consonants_fluent, df_consonants_disfluent):\n",
        "\n",
        "  if len(df_vowels_fluent) + len(df_vowels_disfluent) + len(df_consonants_fluent) + len(df_consonants_disfluent) == 0:\n",
        "    short_v = []\n",
        "    difference_v = []\n",
        "    short_c = []\n",
        "    difference_c = []\n",
        "    longer = []\n",
        "    shorter = []\n",
        "    string1 = (f\"There is no data for this condition.\")\n",
        "    string2 = (\"\")\n",
        "    string3 = (\"\")\n",
        "    string4 = (\"\")\n",
        "  else:\n",
        "    fluent_v = df_vowels_fluent.groupby(\"FluencyStatus\").mean()[\"Syllables\"].mean()\n",
        "    disfluent_v = df_vowels_disfluent.groupby(\"FluencyStatus\").mean()[\"Syllables\"].mean()\n",
        "    difference_v = abs(fluent_v - disfluent_v)\n",
        "\n",
        "    fluent_c = (df_consonants_fluent.groupby(\"FluencyStatus\").mean()[\"Consonants\"]).mean()\n",
        "    disfluent_c = (df_consonants_disfluent.groupby(\"FluencyStatus\").mean()[\"Consonants\"]).mean()\n",
        "    difference_c = abs(fluent_c - disfluent_c)\n",
        "\n",
        "    if fluent_v > disfluent_v:\n",
        "      longer = 'fluent'\n",
        "      shorter = 'disfluent'\n",
        "      short_v = disfluent_v\n",
        "      short_c = disfluent_c\n",
        "    elif disfluent_v > fluent_v:\n",
        "      longer = 'disfluent'\n",
        "      shorter = 'fluent'\n",
        "      short_v = fluent_v\n",
        "      short_c = fluent_c\n",
        "    elif (np.isnan(fluent_v)):\n",
        "      longer = 'disfluent'\n",
        "      shorter = 'fluent'\n",
        "      short_v = fluent_v\n",
        "      short_c = fluent_c\n",
        "    else:\n",
        "      longer = 'fluent'\n",
        "      shorter = 'disfluent'\n",
        "      short_v = disfluent_v\n",
        "      short_c = disfluent_c\n",
        "\n",
        "\n",
        "    string1 = (f\"PWS produced on average {round(fluent_v,2)} syllables in fluent utterance and {round(disfluent_v,2)} syllables in disflunt utterances.\")\n",
        "    string2 = (f\"This means that on average {longer} utterances were {round(difference_v,2)} syllables longer than {shorter} utterances.\")\n",
        "    string3 = (f\"\\nPWS produced on average {round(fluent_c,2)} consonants in fluent utterance and {round(disfluent_c,2)} consonants in disfluent utterances.\")\n",
        "    string4 = (f\"This means that on average {longer} utterances were {round(difference_c,2)} consonants longer than {shorter} utterances.\")\n",
        "\n",
        "  return(string1, string2, string3, string4, short_v, short_c, difference_v, difference_c, longer, shorter)"
      ],
      "metadata": {
        "id": "ucb7p4K0XhCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.7 Match Number of Vowels Across Utterance Types**"
      ],
      "metadata": {
        "id": "5O0tfRnkZxYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_vowels(df_vowels, short_group, cut_v, condition):\n",
        "  if condition == 'frog':\n",
        "    IDs_here = IDs_frog\n",
        "  else:\n",
        "    IDs_here = IDs_reading\n",
        "\n",
        "  IDs_here_pws = []\n",
        "  for ID in IDs_here:\n",
        "    if ID in IDs_pws:\n",
        "      IDs_here_pws.append(ID)\n",
        "\n",
        "  groups = []\n",
        "  for item in df_vowels['FluencyStatus']:\n",
        "    if item == \"fluent\":\n",
        "      groups.append(\"fluent\")\n",
        "    if item == 'disfluent':\n",
        "      groups.append(\"disfluent\")\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  if len(list(set(groups))) < 2:\n",
        "    print(f\"Not enough data in {condition} condition. Matching not possible.\")\n",
        "    syll_col_matched = df_vowels\n",
        "    return(syll_col_matched)\n",
        "  else:\n",
        "    cut_v = round(cut_v)\n",
        "    syll_col_matched = pd.DataFrame()  ## initialize group-level dataframe\n",
        "    participant = pd.DataFrame()  ## initialize participant-level dataframe\n",
        "    if short_group == \"disfluent\":\n",
        "      df_control = df_vowels[df_vowels[\"Group\"]  == \"fluent\"]\n",
        "      df_control.index = range(len(df_control.index)) ## group\n",
        "      for ID in IDs_here_pws:\n",
        "        df_control_ID = df_control[df_control[\"ID\"]  == ID]\n",
        "        df_control_ID.index = range(len(df_control_ID.index))  ### person\n",
        "        BGs = df_control_ID[\"Breath.Group\"].unique()\n",
        "        for BG in BGs:\n",
        "          df_control_ID_BG = df_control_ID[df_control_ID[\"Breath.Group\"]  == BG]\n",
        "          df_control_ID_BG.index = range(len(df_control_ID_BG.index)) ## BG\n",
        "          if len(df_control_ID_BG) >= cut_v:\n",
        "            df_control_ID_BG.drop(df_control_ID_BG.tail(cut_v).index, inplace = True)\n",
        "            participant = participant.append([df_control_ID_BG], ignore_index = True)\n",
        "          else:\n",
        "            participant = participant.append([df_control_ID_BG], ignore_index = True)\n",
        "      else:\n",
        "          df_control = df_vowels[df_vowels[\"Group\"]  == \"disfluent\"]\n",
        "          df_control.index = range(len(df_control.index)) ## group\n",
        "          for ID in IDs_here_pws:\n",
        "            df_control_ID = df_control[df_control[\"ID\"]  == ID]\n",
        "            df_control_ID.index = range(len(df_control_ID.index))  ### person\n",
        "            BGs = df_control_ID[\"Breath.Group\"].unique()\n",
        "            for BG in BGs:\n",
        "              df_control_ID_BG = df_control_ID[df_control_ID[\"Breath.Group\"]  == BG]\n",
        "              df_control_ID_BG.index = range(len(df_control_ID_BG.index)) ## BG\n",
        "              if len(df_control_ID_BG) >= cut_v:\n",
        "                df_control_ID_BG.drop(df_control_ID_BG.tail(cut_v).index, inplace = True)\n",
        "                participant = participant.append([df_control_ID_BG], ignore_index = True)\n",
        "              else:\n",
        "                participant = participant.append([df_control_ID_BG], ignore_index = True)\n",
        "          syll_col_matched = syll_col_matched.append([participant], ignore_index = True) # add participant-level dataframe to group-level dataframe\n",
        "    return(syll_col_matched)"
      ],
      "metadata": {
        "id": "wVqayl9aXg-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.8 Match Number of Consonants Across Utterance Types**"
      ],
      "metadata": {
        "id": "MG3RTaLqZ2cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_consonants(df_consonants, short_group, cut_c, condition):\n",
        "  if condition == 'frog':\n",
        "    IDs_here = IDs_frog\n",
        "  else:\n",
        "    IDs_here = IDs_reading\n",
        "\n",
        "  IDs_here_pws = []\n",
        "  for ID in IDs_here:\n",
        "    if ID in IDs_pws:\n",
        "      IDs_here_pws.append(ID)\n",
        "\n",
        "  groups = []\n",
        "  for item in df_consonants['FluencyStatus']:\n",
        "    if item == \"fluent\":\n",
        "      groups.append(\"fluent\")\n",
        "    if item == 'disfluent':\n",
        "      groups.append(\"disfluent\")\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  if len(list(set(groups))) < 2:\n",
        "    print(f\"Not enough data in {condition} condition. Matching not possible.\")\n",
        "    cons_col_matched = df_consonants\n",
        "    return(cons_col_matched)\n",
        "  else:\n",
        "    cut_c = round(cut_c)\n",
        "    cons_col_matched = pd.DataFrame()  ## initialize group-level dataframe\n",
        "    participant = pd.DataFrame()  ## initialize participant-level dataframe\n",
        "    if short_group == \"disfluent\":\n",
        "      df_control = df_consonants[df_consonants[\"Group\"]  == \"fluent\"]\n",
        "      df_control.index = range(len(df_control.index)) ## group\n",
        "      for ID in control_IDs:\n",
        "        df_control_ID = df_control[df_control[\"ID\"]  == ID]\n",
        "        df_control_ID.index = range(len(df_control_ID.index))  ### person\n",
        "        BGs = df_control_ID[\"Breath.Group\"].unique()\n",
        "        for BG in BGs:\n",
        "          df_control_ID_BG = df_control_ID[df_control_ID[\"Breath.Group\"]  == BG]\n",
        "          df_control_ID_BG.index = range(len(df_control_ID_BG.index)) ## BG\n",
        "          if len(df_control_ID_BG) >= cut_c:\n",
        "            df_control_ID_BG.drop(df_control_ID_BG.tail(cut_c).index, inplace = True)\n",
        "            participant = participant.append([df_control_ID_BG], ignore_index = True)\n",
        "          else:\n",
        "            participant = participant.append([df_control_ID_BG], ignore_index = True)\n",
        "        cons_col_matched = cons_col_matched.append([participant], ignore_index = True) # add participant-level dataframe to group-level dataframe\n",
        "    else:\n",
        "      df_control = df_consonants[df_consonants[\"Group\"]  == \"disfluent\"]\n",
        "      df_control.index = range(len(df_control.index)) ## group\n",
        "      for ID in IDs_pws:\n",
        "        df_control_ID = df_control[df_control[\"ID\"]  == ID]\n",
        "        df_control_ID.index = range(len(df_control_ID.index))  ### person\n",
        "        BGs = df_control_ID[\"Breath.Group\"].unique()\n",
        "        for BG in BGs:\n",
        "          df_control_ID_BG = df_control_ID[df_control_ID[\"Breath.Group\"]  == BG]\n",
        "          df_control_ID_BG.index = range(len(df_control_ID_BG.index)) ## BG\n",
        "          if len(df_control_ID_BG) >= cut_c:\n",
        "            df_control_ID_BG.drop(df_control_ID_BG.tail(cut_c).index, inplace = True)\n",
        "            participant = participant.append([df_control_ID_BG], ignore_index = True)\n",
        "          else:\n",
        "            participant = participant.append([df_control_ID_BG], ignore_index = True)\n",
        "      cons_col_matched = cons_col_matched.append([participant], ignore_index = True) # add participant-level dataframe to group-level dataframe\n",
        "  return(cons_col_matched)"
      ],
      "metadata": {
        "id": "r9JD7G44Xg7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.9 Count Utterances**"
      ],
      "metadata": {
        "id": "annRKtOmaDse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def countUtterances(df):\n",
        "  sum = 0\n",
        "  for ID in df[\"ID\"].unique():\n",
        "    subset_ID = df[df[\"ID\"] == ID]\n",
        "    sum = sum + len(subset_ID[\"Breath.Group\"].unique())\n",
        "  return(sum)"
      ],
      "metadata": {
        "id": "vYMkSzO_Xgz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.10 Test if Remaining Vowel Difference is Significant**"
      ],
      "metadata": {
        "id": "xELhzEFPaHsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ttest_vowelDifference(matched_participant_df, shorter_group):\n",
        "  if len(matched_participant_df) == 0:\n",
        "      string11 = (\"There is no data for this condition.\")\n",
        "      return(string11)\n",
        "  if shorter_group == \"fluent\":\n",
        "    longer_group = \"disfluent\"\n",
        "  else:\n",
        "    longer_group = \"fluent\"\n",
        "\n",
        "  ttest_fluent = matched_participant_df[matched_participant_df[\"FluencyStatus\"] == \"Fluent\"]\n",
        "  ttest_disfluent = matched_participant_df[matched_participant_df[\"FluencyStatus\"] == \"Disfluent\"]\n",
        "\n",
        "  ttest_fluent = ttest_fluent.dropna()\n",
        "  ttest_disfluent = ttest_fluent.dropna()\n",
        "\n",
        "  fluent_v =  ttest_fluent.mean()[\"Syllables\"]\n",
        "  disfluent_v =  ttest_disfluent.mean()[\"Syllables\"]\n",
        "\n",
        "  difference_v_matched = fluent_v - disfluent_v\n",
        "\n",
        "  df_v_matched_test = stats.ttest_ind(np.array(ttest_fluent['Syllables'].astype(float)),\n",
        "                      np.array(ttest_fluent['Syllables']).astype(float))\n",
        "  if df_v_matched_test[1] >= 0.05:\n",
        "    level = \"insignificant\"\n",
        "  elif df_v_matched_test[1] < 0.05:\n",
        "    level = \"significant\"\n",
        "  else:\n",
        "    level = 'undetermined'\n",
        "  string11 = (f\"{longer_group} utterances that were longer than the mean average length of {shorter_group} utterances were shortened. \\nAfter matching, PWS had {round(fluent_v,2)} vowels per fluent utterance, while they had {round(disfluent_v,2)} per disfluent utterance. \\nThe difference is reduced to {abs(round(difference_v_matched,2))}, which is statistically {level} (p = {round(df_v_matched_test[1],2)}).\")\n",
        "\n",
        "  return(string11)"
      ],
      "metadata": {
        "id": "vU6gSDg_Xglw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.11 Test if Remaining Consonant Difference is Significant**"
      ],
      "metadata": {
        "id": "J406kMozaR03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ttest_conDifference(matched_participant_df, shorter_group):\n",
        "  if len(matched_participant_df) == 0:\n",
        "    string11 = (\"There is no data for this condition.\")\n",
        "    return(string11)\n",
        "  if shorter_group == \"fluent\":\n",
        "    longer_group = \"disfluent\"\n",
        "  else:\n",
        "    longer_group = \"fluent\"\n",
        "  ttest_disfluent = matched_participant_df[matched_participant_df[\"FluencyStatus\"] == \"Disfluent\"]\n",
        "  ttest_fluent = matched_participant_df[matched_participant_df[\"FluencyStatus\"] == \"Fluent\"]\n",
        "\n",
        "  ttest_disfluent = ttest_disfluent.dropna()\n",
        "  ttest_fluent = ttest_fluent.dropna()\n",
        "\n",
        "  disfluent_c =  ttest_disfluent.mean()[\"Consonants\"]\n",
        "  fluent_c =  ttest_fluent.mean()[\"Consonants\"]\n",
        "\n",
        "  difference_c_matched = disfluent_c - fluent_c\n",
        "\n",
        "  df_c_matched_test = stats.ttest_ind(np.array(ttest_disfluent['Consonants']).astype(float),\n",
        "                        np.array(ttest_fluent['Consonants']).astype(float))\n",
        "  if df_c_matched_test[1] >= 0.05:\n",
        "    level = \"insignificant\"\n",
        "  elif df_c_matched_test[1] < 0.05:\n",
        "    level = \"significant\"\n",
        "  else:\n",
        "    level = \"undetermined\"\n",
        "  string11 = (f\"{longer_group} utterances that were longer than the mean average length of {shorter_group} utterances, were shortened. \\nAfter matching, fluent utterances had {round(fluent_c,2)} consonants per utterance, while disfluent utterances had {round(disfluent_c,2)} per utterance. \\nThe difference is reduced to {abs(round(difference_c_matched,2))}, which is statistically {level} (p = {round(df_c_matched_test[1],2)}).\")\n",
        "\n",
        "  return(string11)"
      ],
      "metadata": {
        "id": "PR76u6RFaHQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Match Across Utterance Types (Fluent vs. Disfluent)**"
      ],
      "metadata": {
        "id": "dX3aGKjzbPPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1 Pre-Matching**"
      ],
      "metadata": {
        "id": "Fyzw-CkSzLYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add new column to dataframe that denotes participant's group membership\n",
        "frog = assign_group(frog)\n",
        "reading = assign_group(reading)"
      ],
      "metadata": {
        "id": "B2PZPWVFbznv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# account for case differences in annotation\n",
        "for i in range (0, len(frog)):\n",
        "  frog[\"FluencyStatus\"][i] = frog[\"FluencyStatus\"][i].lower().strip()\n",
        "for i in range (0, len(reading)):\n",
        "  reading[\"FluencyStatus\"][i] = reading[\"FluencyStatus\"][i].lower().strip()"
      ],
      "metadata": {
        "id": "xGGdt3OEb8JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# exclude control participants from further analysis\n",
        "frog_pws = frog[frog[\"Group\"] == \"PWS\"]\n",
        "frog_pws.index = range(len(frog_pws.index))\n",
        "reading_pws = reading[reading[\"Group\"] == \"PWS\"]\n",
        "reading_pws.index = range(len(reading_pws.index))"
      ],
      "metadata": {
        "id": "M_wkCRu7bSoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count vowels per utterance\n",
        "frog_vowels_pws, pre_frog_vowel_avg_across_BG = count_vowels(frog_pws, \"frog\")\n",
        "reading_vowels_pws, pre_reading_vowel_avg_across_BG = count_vowels(reading_pws, \"reading\")"
      ],
      "metadata": {
        "id": "nzytf5f7bSka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count consonants per utterance\n",
        "[frog_consonants_pws, pre_frog_consonant_avg_across_BG]  = count_consonants(frog_pws,'frog')\n",
        "[reading_consonants_pws, pre_reading_consonants_avg_across_BG] = count_consonants(reading_pws,'reading')"
      ],
      "metadata": {
        "id": "km5qT76zbShq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#average # vowels per breath group for each participant\n",
        "frog_participant_vowel_avg_pws = participant_vowel_avg(frog_vowels_pws,'frog')\n",
        "reading_participant_vowel_avg_pws = participant_vowel_avg(reading_vowels_pws,'reading')"
      ],
      "metadata": {
        "id": "VqioPCGLbSfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#average # consonants per breath group for each participant\n",
        "frog_participant_cons_avg_pws = participant_consonant_avg(frog_consonants_pws,'frog')\n",
        "reading_participant_cons_avg_pws = participant_consonant_avg(reading_consonants_pws,'reading')"
      ],
      "metadata": {
        "id": "3--upHLgbScD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#average # per breath group for each participant and fluency status in frog\n",
        "if len(frog_participant_vowel_avg_pws) > 1:\n",
        "  frog_participant_vowels_avg_pws_fluent = frog_participant_vowel_avg_pws[frog_participant_vowel_avg_pws[\"FluencyStatus\"] == \"Fluent\"]\n",
        "  frog_participant_vowels_avg_pws_disfluent = frog_participant_vowel_avg_pws[frog_participant_vowel_avg_pws[\"FluencyStatus\"] == \"Disfluent\"]\n",
        "  frog_participant_consonants_avg_pws_fluent = frog_participant_cons_avg_pws[frog_participant_cons_avg_pws[\"FluencyStatus\"] == \"Fluent\"]\n",
        "  frog_participant_consonants_avg_pws_disfluent = frog_participant_cons_avg_pws[frog_participant_cons_avg_pws[\"FluencyStatus\"] == \"Disfluent\"]\n",
        "else:\n",
        "  frog_participant_vowels_avg_pws_fluent = pd.DataFrame()\n",
        "  frog_participant_vowels_avg_pws_disfluent = pd.DataFrame()\n",
        "  frog_participant_consonants_avg_pws_fluent = pd.DataFrame()\n",
        "  frog_participant_consonants_avg_pws_disfluent = pd.DataFrame()"
      ],
      "metadata": {
        "id": "iEP1ZF_hYaZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#average # per breath group for each participant and fluency status in reading\n",
        "if len(reading_participant_vowel_avg_pws) > 1:\n",
        "  reading_participant_vowels_avg_pws_fluent = reading_participant_vowel_avg_pws[reading_participant_vowel_avg_pws[\"FluencyStatus\"] == \"Fluent\"]\n",
        "  reading_participant_vowels_avg_pws_disfluent = reading_participant_vowel_avg_pws[reading_participant_vowel_avg_pws[\"FluencyStatus\"] == \"Disfluent\"]\n",
        "  reading_participant_consonants_avg_pws_fluent = reading_participant_cons_avg_pws[reading_participant_cons_avg_pws[\"FluencyStatus\"] == \"Fluent\"]\n",
        "  reading_participant_consonants_avg_pws_disfluent = reading_participant_cons_avg_pws[reading_participant_cons_avg_pws[\"FluencyStatus\"] == \"Disfluent\"]\n",
        "else:\n",
        "  reading_participant_vowels_avg_pws_fluent = pd.DataFrame()\n",
        "  reading_participant_vowels_avg_pws_disfluent = pd.DataFrame()\n",
        "  reading_participant_consonants_avg_pws_fluent = pd.DataFrame()\n",
        "  reading_participant_consonants_avg_pws_disfluent = pd.DataFrame()"
      ],
      "metadata": {
        "id": "rw8GUCUPX6pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare group averages of number of vowels per utterance and number of consonants per utterance\n",
        "# frog\n",
        "string1, string2, string3, string4, short_v_frog_pws, short_c_frog_pws, difference_v_frog_pws, difference_c_frog_pws, long_group_frog, short_group_frog = compare_groups(frog_participant_vowels_avg_pws_fluent, frog_participant_vowels_avg_pws_disfluent, frog_participant_consonants_avg_pws_fluent, frog_participant_consonants_avg_pws_disfluent)\n",
        "\n",
        "print(string1)\n",
        "print(string2)\n",
        "print(string3)\n",
        "print(string4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS79yUGZXQoy",
        "outputId": "44dd7e45-ad39-4a39-fc64-c173f66c54f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PWS produced on average 6.66 syllables in fluent utterance and nan syllables in disflunt utterances.\n",
            "This means that on average fluent utterances were nan syllables longer than disfluent utterances.\n",
            "\n",
            "PWS produced on average 9.95 consonants in fluent utterance and nan consonants in disfluent utterances.\n",
            "This means that on average fluent utterances were nan consonants longer than disfluent utterances.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare group averages of number of vowels per utterance and number of consonants per utterance\n",
        "# reading\n",
        "string1, string2, string3, string4, short_v_read_pws, short_c_read_pws, difference_v_read_pws, difference_c_read_pws, long_group_read, short_group_read = compare_groups(reading_participant_vowels_avg_pws_fluent, reading_participant_vowels_avg_pws_disfluent, reading_participant_consonants_avg_pws_fluent, reading_participant_consonants_avg_pws_disfluent)\n",
        "\n",
        "print(string1)\n",
        "print(string2)\n",
        "print(string3)\n",
        "print(string4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymvO80zMaohU",
        "outputId": "fdb4d1fa-3ead-40a8-efee-c13a6d7cef64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is no data for this condition.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2. Matching**"
      ],
      "metadata": {
        "id": "D7dcFT74dIHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# match number of vowels per utterance by cutting long control utterances by the difference in average vowel number between groups\n",
        "matched_vowels_read_pws = match_vowels(reading_vowels_pws, short_group_read,  difference_v_read_pws,'reading')\n",
        "matched_vowels_frog_pws = match_vowels(frog_vowels_pws, short_group_frog, difference_v_frog_pws,'frog')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NV-qA5RdOxh",
        "outputId": "50e84d7d-10f3-452d-f117-5ab8f2e4179b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not enough data in reading condition. Matching not possible.\n",
            "Not enough data in frog condition. Matching not possible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# match number of consonants per utterance by cutting long control utterances by the difference in average consonant number between groups\n",
        "matched_consonants_read_pws = match_consonants(reading_consonants_pws, short_group_read, difference_c_read_pws,'reading')\n",
        "matched_consonants_frog_pws = match_consonants(frog_consonants_pws, short_group_frog, difference_c_frog_pws,'frog')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCNU8JM_d4hZ",
        "outputId": "1030b64a-c574-4c31-c3ce-5204f147f105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not enough data in reading condition. Matching not possible.\n",
            "Not enough data in frog condition. Matching not possible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if (difference_c_read_pws == []) or (np.isnan(difference_c_read_pws)):\n",
        "  difference_c_read_pws = 0\n",
        "if (difference_c_frog_pws == []) or (np.isnan(difference_c_frog_pws)):\n",
        "  difference_c_frog_pws = 0\n",
        "if (difference_v_read_pws == []) or (np.isnan(difference_v_read_pws)):\n",
        "  difference_v_read_pws = 0\n",
        "if (difference_v_frog_pws == []) or (np.isnan(difference_v_frog_pws)):\n",
        "  difference_v_frog_pws = 0\n",
        "if long_group_read == []:\n",
        "  long_group_read = \"No\"\n",
        "if long_group_frog == []:\n",
        "  long_group_frog = \"No\"\n",
        "\n",
        "print(f\"{long_group_read} utterances were cut by {round(difference_v_read_pws)} vowels and {round(difference_c_read_pws)} consonants in the reading condition.\")\n",
        "print(f\"{long_group_frog} utterances were cut by {round(difference_v_frog_pws)} vowels and {round(difference_c_frog_pws)} consonants in the frog condition.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw_PjaXqe7xq",
        "outputId": "45891f44-0dd4-4783-ce66-f8604efaacc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No utterances were cut by 0 vowels and 0 consonants in the reading condition.\n",
            "fluent utterances were cut by 0 vowels and 0 consonants in the frog condition.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3. Post-Matching**"
      ],
      "metadata": {
        "id": "JE95ubxPgbYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# combined shortened dataframe with original dataframe\n",
        "# frog\n",
        "if short_group_frog == \"fluent\":\n",
        "  matched_vowels_frog_pws = matched_vowels_frog_pws.append(frog_vowels_pws[frog_vowels_pws[\"FluencyStatus\"]==\"fluent\"], ignore_index=True)\n",
        "  matched_consonants_frog_pws =  matched_consonants_frog_pws.append(frog_consonants_pws[frog_consonants_pws[\"FluencyStatus\"]==\"fluent\"], ignore_index=True)\n",
        "elif short_group_frog == \"disfluent\":\n",
        "  matched_vowels_frog_pws = matched_vowels_frog_pws.append(frog_vowels_pws[frog_vowels_pws[\"FluencyStatus\"]==\"disfluent\"], ignore_index=True)\n",
        "  matched_consonants_frog_pws = matched_consonants_frog_pws.append(frog_consonants_pws[frog_consonants_pws[\"FluencyStatus\"]==\"disfluent\"], ignore_index=True)"
      ],
      "metadata": {
        "id": "0iNpUQbWfSo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined shortened dataframe with original dataframe\n",
        "# reading\n",
        "if short_group_read == \"fluent\":\n",
        "  matched_vowels_reading_pws = matched_vowels_read_pws.append(reading_vowels_pws[reading_vowels_pws[\"FluencyStatus\"]==\"fluent\"], ignore_index=True)\n",
        "  matched_consonants_reading_pws =  matched_consonants_read_pws.append(reading_consonants_pws[reading_consonants_pws[\"FluencySTatus\"]==\"fluent\"], ignore_index=True)\n",
        "elif short_group_read == \"disfluent\":\n",
        "  matched_vowels_reading_pws = matched_vowels_read_pws.append(reading_vowels_pws[reading_vowels_pws[\"FLuencyStatus\"]==\"disfluent\"], ignore_index=True)\n",
        "  matched_consonants_reading_pws = matched_consonants_read_pws.append(reading_consonants_pws[reading_consonants_pws[\"FluencyStatus\"]==\"disfluent\"], ignore_index=True)"
      ],
      "metadata": {
        "id": "27LMjKxohOSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count vowels per utterance after matching\n",
        "[post_frog_vowels_pws, post_frog_vowel]  = count_vowels(matched_vowels_frog_pws,'frog')\n",
        "[post_reading_vowels_pws, post_reading_vowel]  = count_vowels(matched_vowels_read_pws,'reading')"
      ],
      "metadata": {
        "id": "FS6_knyBiwL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# average # vowels per breath group for each participant after matching\n",
        "post_frog_participant_vowels_avg_pws = participant_vowel_avg(post_frog_vowels_pws, 'frog')\n",
        "post_reading_participant_vowels_avg_pws = participant_vowel_avg(post_reading_vowels_pws,'reading')"
      ],
      "metadata": {
        "id": "ZfnIY7V_iyTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count consonants per utterance after matching\n",
        "[post_frog_consonants_pws, post_frog_consonant]  = count_consonants(matched_consonants_frog_pws, 'frog')\n",
        "[post_reading_consonants_pws, post_reading_consonant]  = count_consonants(matched_consonants_read_pws,'reading')"
      ],
      "metadata": {
        "id": "KCgG_DeRi0rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#average # consonants per breath group for each participant after matching\n",
        "post_frog_participant_cons_avg_pws = participant_consonant_avg(post_frog_consonants_pws, 'frog')\n",
        "post_reading_participant_cons_avg_pws = participant_consonant_avg(post_reading_consonants_pws,'reading')"
      ],
      "metadata": {
        "id": "g6MLAx8vi3Hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#average # per breath group for each participant and fluency status in frog\n",
        "if len(post_frog_participant_vowels_avg_pws) > 1:\n",
        "  post_frog_participant_vowels_avg_pws_fluent = post_frog_participant_vowels_avg_pws[post_frog_participant_vowels_avg_pws[\"FluencyStatus\"] == \"Fluent\"]\n",
        "  post_frog_participant_vowels_avg_pws_disfluent = post_frog_participant_vowels_avg_pws[post_frog_participant_vowels_avg_pws[\"FluencyStatus\"] == \"Disfluent\"]\n",
        "  post_frog_participant_consonants_avg_pws_fluent = post_frog_participant_cons_avg_pws[post_frog_participant_cons_avg_pws[\"FluencyStatus\"] == \"Fluent\"]\n",
        "  post_frog_participant_consonants_avg_pws_disfluent = post_frog_participant_cons_avg_pws[post_frog_participant_cons_avg_pws[\"FluencyStatus\"] == \"Disfluent\"]\n",
        "else:\n",
        "  post_frog_participant_vowels_avg_pws_fluent = pd.DataFrame()\n",
        "  post_frog_participant_vowels_avg_pws_disfluent = pd.DataFrame()\n",
        "  post_frog_participant_consonants_avg_pws_fluent = pd.DataFrame()\n",
        "  post_frog_participant_consonants_avg_pws_disfluent = pd.DataFrame()"
      ],
      "metadata": {
        "id": "A3T1PxPvjkGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#average # per breath group for each participant and fluency status in reading\n",
        "if len(post_reading_participant_vowels_avg_pws) > 1:\n",
        "  post_reading_participant_vowels_avg_pws_fluent = post_reading_participant_vowels_avg_pws[post_reading_participant_vowels_avg_pws[\"FluencyStatus\"] == \"Fluent\"]\n",
        "  post_reading_participant_vowels_avg_pws_disfluent = post_reading_participant_vowels_avg_pws[post_reading_participant_vowels_avg_pws[\"FluencyStatus\"] == \"Disfluent\"]\n",
        "  post_reading_participant_consonants_avg_pws_fluent = post_reading_participant_cons_avg_pws[post_reading_participant_cons_avg_pws[\"FluencyStatus\"] == \"Fluent\"]\n",
        "  post_reading_participant_consonants_avg_pws_disfluent = post_reading_participant_cons_avg_pws[post_reading_participant_cons_avg_pws[\"FluencyStatus\"] == \"Disfluent\"]\n",
        "else:\n",
        "  post_reading_participant_vowels_avg_pws_fluent = pd.DataFrame()\n",
        "  post_reading_participant_vowels_avg_pws_disfluent = pd.DataFrame()\n",
        "  post_reading_participant_consonants_avg_pws_fluent = pd.DataFrame()\n",
        "  post_reading_participant_consonants_avg_pws_disfluent = pd.DataFrame()"
      ],
      "metadata": {
        "id": "OeYBqh-MkIz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare group averages of number of vowels per utterance and number of consonants per utterance\n",
        "# frog\n",
        "string1, string2, string3, string4, short_v_frog_pws, short_c_frog_pws, difference_v_frog_pws, difference_c_frog_pws, long_group_frog, short_group_frog = compare_groups(post_frog_participant_vowels_avg_pws_fluent, post_frog_participant_vowels_avg_pws_disfluent, post_frog_participant_consonants_avg_pws_fluent, post_frog_participant_consonants_avg_pws_disfluent)\n",
        "\n",
        "print(string1)\n",
        "print(string2)\n",
        "print(string3)\n",
        "print(string4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LsuxN1Ti39B",
        "outputId": "1f6c057c-2363-4b36-8e96-5452947a059e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PWS produced on average 6.66 syllables in fluent utterance and nan syllables in disflunt utterances.\n",
            "This means that on average fluent utterances were nan syllables longer than disfluent utterances.\n",
            "\n",
            "PWS produced on average 9.95 consonants in fluent utterance and nan consonants in disfluent utterances.\n",
            "This means that on average fluent utterances were nan consonants longer than disfluent utterances.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare group averages of number of vowels per utterance and number of consonants per utterance\n",
        "# reading\n",
        "string1, string2, string3, string4, short_v_read_pws, short_c_read_pws, difference_v_read_pws, difference_c_read_pws, long_group_read, short_group_read = compare_groups(post_reading_participant_vowels_avg_pws_fluent, post_reading_participant_vowels_avg_pws_disfluent, post_reading_participant_consonants_avg_pws_fluent, post_reading_participant_consonants_avg_pws_disfluent)\n",
        "\n",
        "print(string1)\n",
        "print(string2)\n",
        "print(string3)\n",
        "print(string4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRfqDn-yi3yB",
        "outputId": "b40c6316-e1b6-44e5-977d-bac3670f5773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is no data for this condition.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Significance After Matching**"
      ],
      "metadata": {
        "id": "zIAoy9SNrnUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test if average number of vowels per breathgroup is still significantly different between utterance types after matching\n",
        "# frog\n",
        "string1 = ttest_vowelDifference(post_frog_participant_vowels_avg_pws, short_group_frog)\n",
        "print(string1)"
      ],
      "metadata": {
        "id": "cOEb3rvXrrgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test if average number of vowels per breathgroup is still significantly different between utterance types after matching\n",
        "# reading\n",
        "string2 = ttest_vowelDifference(post_reading_participant_vowels_avg_pws, short_group_read)\n",
        "print(string2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13_Qsp8UruMo",
        "outputId": "45fef7f6-847f-439d-9dc3-e581be0b1671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is no data for this condition.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test if average number of consonants per breathgroup is still significantly different between utterance types after matching\n",
        "# frog\n",
        "string1 = ttest_conDifference(post_frog_participant_cons_avg_pws, short_group_frog)\n",
        "print(string1)"
      ],
      "metadata": {
        "id": "jirWYUbjrxu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test if average number of consonants per breathgroup is still significantly different between utterance types after matching\n",
        "# reading\n",
        "string2 = ttest_conDifference(post_reading_participant_cons_avg_pws, short_group_read)\n",
        "print(string2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j812jD0Or17A",
        "outputId": "24dca0fd-2a34-473e-a6c3-62b1a9183cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is no data for this condition.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Count Utterances in Each Category**"
      ],
      "metadata": {
        "id": "WZoTBrcavzgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pws_frog_fluent = post_frog_vowels_pws[post_frog_vowels_pws[\"FluencyStatus\"] == \"fluent\"]\n",
        "pws_frog_disfluent = post_frog_vowels_pws[post_frog_vowels_pws[\"FluencyStatus\"] == \"disfluent\"]\n",
        "pws_read_fluent = post_reading_vowels_pws[post_reading_vowels_pws[\"FluencyStatus\"] == \"fluent\"]\n",
        "pws_read_disfluent = post_reading_vowels_pws[post_reading_vowels_pws[\"FluencyStatus\"] == \"disfluent\"]"
      ],
      "metadata": {
        "id": "2ewSE5w5v3Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_reading_pws_disfluent = countUtterances(pws_read_disfluent)\n",
        "number_frog_pws_disfluent = countUtterances(pws_frog_disfluent)\n",
        "number_reading_pws_fluent = countUtterances(pws_read_fluent)\n",
        "number_frog_pws_fluent = countUtterances(pws_frog_fluent)"
      ],
      "metadata": {
        "id": "XB5uJQV2v7z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"There are {number_reading_pws_fluent} fluent utterances in the reading condition and {number_reading_pws_disfluent} disfluent utterances.\")\n",
        "print(f\"\\nThere are {number_frog_pws_fluent} fluent utterances in the frog condition and {number_frog_pws_disfluent} disfluent utterances.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvqI59pcv9_U",
        "outputId": "9c1ef0c6-f5d1-4faf-a2cb-e9214f7eafae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 0 fluent utterances in the reading condition and 0 disfluent utterances.\n",
            "\n",
            "There are 41 fluent utterances in the frog condition and 0 disfluent utterances.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Save**"
      ],
      "metadata": {
        "id": "I46YpjcyxHBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/ATAS_Plus/Duration_Metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJbpbgfpxbda",
        "outputId": "cbd7d1d2-1a45-4d8d-cab5-b46bd5167faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ATAS_Plus/Duration_Metrics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = \"3.MLU_Matched\"\n",
        "\n",
        "if os.path.exists(dir) == False:\n",
        "  os.mkdir(dir)"
      ],
      "metadata": {
        "id": "8x7by6htxdq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/ATAS_Plus/Duration_Metrics/3.MLU_Matched/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANr3X7UUxfb-",
        "outputId": "010d715e-b5ed-4486-af47-e07e877e1166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ATAS_Plus/Duration_Metrics/3.MLU_Matched\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# export\n",
        "post_reading_vowels_pws.to_excel(\"fluencyMatched_vowels_reading_pws.xlsx\")\n",
        "post_reading_consonants_pws.to_excel(\"fluencyMatched_consonants_reading_pws.xlsx\")\n",
        "post_frog_vowels_pws.to_excel(\"fluencyMatched_vowels_frog_pws.xlsx\")\n",
        "post_frog_consonants_pws.to_excel(\"fluencyMatched_consonants_frog_pws.xlsx\")"
      ],
      "metadata": {
        "id": "w4TBo_0TxhhN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}